* Principles
** Basic Representation
   - From CHR-based typing: Translate expressions into predicates on types.
   - Difference: all types of words are binary, relating stacks to stacks
     (perhaps the same but basically always expanding the arrows?)
   -
** Types
   - Proposition-based typing: Refinement types (logical typing?)
   - Basic predicates about types, e.g. ~P{ Instance ?x number }
   - [logicdiss] Occurrence typing without semantic subtyping is crap



*** Quotations
    - represented by effect predicates, which act as instantiation templates
      when a call is registered.  These are inferred from the body.  Currently,
      all references to the internal variables should be removed once the effect
      is constructed
      - Reasoning back into the quotation constraints is thus not really possible
        after it has been created.
      - TODO: Currying?
    - Duplication is currently handled symbolically, e.g. duplication of a
      callable is referential, and any per-call differentiation happens via
      instantiation
*** Intersection/Union semantics
    - Set of constraints is a conjunction of predicates, so there are two
      possibilities to interpret something like
      Type(x) = integer ∧ Type(x) = string
      1. The variable x must be of the intersection type integer∧string
      2. The variable x must be able to hold values of type integer as well as
         values of type string.


      This is a basic conflict of meaning: "The value must be both a string and
      an integer" vs. "The variable must be able to hold both a string and an integer".


      In a modal interpretation, this could maybe be read as, respectively:
      1. Possible(Type(x) = integer) ∧ Possible(Type(x) = string)
      2. Necessary(Type(x) = (integer∪string))


      Without introducing ambiguities in the set semantics.  This would make
      sense if the types as value-sets specify the value range, while "building"
      up the type is unterstood as a composition of bottom-bounded sets?

      The different ways to construct these is already defined by the different
      ways to interpret the presence of multiple effect definitions.
      Declarations serve to actively cut the possiblity space, while alternative
      executions build up the possibility space.

      Approach:
      - Interpret conjunction of declaration constraints as intersection types
      - Interpret conjunction of instance constraints as union-creating types


      From the conditional structure, could we "force" unions to be indexed?
*** Occurrence/Refinement Typing
    Big difference to "regular" type-logic connections:
    types, propositions and terms are distinct.  Propositions state claims about
    runtime environment, relating types and variables.


    - Approach from [tobin-hochstadt2010logical_types]: Propositional types
    - For each type, a propositional expression containing the following
      information is stated:
      - Type: T, N, #t, #f, union(⋃)
      - latent propositions:
        - case 1: expression evaluates to boolean true
        - case 2: expression evaluates to boolean false
        - Propositions can be tt, ff, atomic: type_(x), implication,
          conjunction, disjunction
        - *object* This one is interesting.  It defines how to access the part
          of the runtime environment which the proposition refers to.  Base
          system: variable, empty object
**** RTR variant:
     - expression is assigned type-result e: <τ, p, q, o> where
       - e has type τ
       - if e evaluates to non-false, then p holds
       - if e evaluates to false, then q holds
       - e's value corresponds to symbolic object o
*** Predicate Typing/Branches

    Always problem: keep compositionality.
    Equal sign ~=~ means assignment from right to left
    Idea: Distinction: value properties, variable properties, e.g.

    ~dup number? [ swap ] when~

    - x2 = dup(x1)
    - x3 = number?(x2)
    - True{ x3 } -->
      - (x4, x5) = (x1, x0) (just here we know that there was another input underneath)
    - False{ x3 } -->
    - Some way to handle branch merge
    - Stack after cond merge: (x6, x7)
    - "Return"(x6, x7)


    A name like ~x4~ represents a stack element in a certain state.
    Regarding branching/return, two alternatives:
    1. Always create new vars, have phi nodes on join
    2. Split store, carry same var into different branches, rename outputs
       accordingly


    
    With regards to the target output, we will most likely need to have the same
    var ref in different branches, since pretty much no language supports
    anything else in terms of defining return values.  Regarding the type of a
    ref like ~x2~: Inside a branch, it is the most precise possible.  On the top
    level (since we assume it is not a local variable), we need to consider that
    the "container" variable must be able to hold both possible values.

    So there are predicates like
    - ~True{ x3 } --> { Instance x2 number }~ and
    - ~False{ x3 } --> { Instance x3 not{ number } }~
    at the same time.  In the context of CHR, this is to be taken as a
    conjunction.  So all statements must be true at the same time.  This creates
    some interpretational difficulties.

    Suppose we have that in one branch: ~{Instance x3 number}~, and in
    another branch ~{Instance x3 fixnum}~.  If we lift this up into the parent
    context, then if we say ~{Instance x3 number}~ is certainly always true.
    But this corresponds to the set-theoretic union operation, and not the
    set-theoretic intersection operation, which would be the natural equivalent
    of a conjunction of logical constraints.

**** Predicate functions
     Problem: During inference, if we infer effects, we infer dependent types.
     These are sets of constraints.  Once inferred, they can simply be applied.
     - Effects are scoped sets of predicates that we want to apply again
     - Effects must also be inferred
     - So two things happen in parallel:
       1. A ~call~ indicates that a value must have an effect
       2. An effect that is present for a ~call~ must be applied to the values

       But these things can happen "in" parallel.  So if we apply an effect, but
       notice that actually we are refininig it in the process, we would have to
       re-apply it.  Effectively, this would amount to a fixpoint search for the
       set of constraints in the predicate function body, which is determined by
       the call site itself...
** Stack Effects
   There is a fundamental difference between e.g. these two approaches regarding
   shuffle words:
   1. ~: swap ( a b -- b a )~
   2. ~: swap ( a b -- x y )~ | Is{ x b } Is{ y a }


   The first one simply acts as renaming operations in the theory of Herbrand
   terms when applying CHR rules.
   The second one has "interface" semantics in the sense that there are
   different names for input variables and output variables, and their
   connection has to be made implicit.

   For the first case, we basically simplify all ~Is{ a b }~ constraints to
   unify ~a~ and ~b~, while for the second one, we only do that for transitive
   pairs of this predicate.  Note that we can recover the second representation
   from the first by simply wrapping the output again in an ~Is~ predicate, but
   preventing the same simplifaction happending, either by restricting the
   context, or using a variant of ~Is~ that is excluded from that simplification
   rule.  This would then be a stable ~Alias~ predicate in the general case, or
   a kind of ~Return~ predicate when interpreted as the last in a chain of predicates...


* Constraint Handling Rules
  - used to express properties about types
  - Most basic: Effect types
    - Declare the input/output parms of a function
    - optionally tagged with a conditional case for keeping track of type
      disjunctions
    - "Body" containing constraints about the parameter
    - effectively a ∀i,o:... construct
    - The original approach is (ref chr-type-paper) to translate the program
      into rules for each occurrence.  Important: These are simplification rules
      at the call sites
      - This implementation originally also did that by computing the
        simplification rules on the fly, but switched to simulating the
        simplification instantiation
      - One reason: bi-directionality.  We want to be able to add information
        about an instantiation rule after a function is encountered (Most
        important example: currying).
      - Other problem: overloading behavior. (See also [stuckey2005], but for
        static typing).  We effectively infer different effect types (which
        correspond to different rules) for different possibilities of
        execution.  For static typing, the correct one is chosen based on the
        shape of the argument term.  However, this only works for syntactic
        typing, where the argument type is completely determined by the
        structure of the term.  For semantic subtyping, whether a rule would
        succeed can only be seen by testing the instantiation, hence the need
        for CHR⋁ semantics.
      - In other words, it can be the case that while during runtime, only one
        version will be executed, during compile time, we must deal with the
        fact that we don't know which one.  Thus, we consider all applicable
        ones.  Note that this property is "contagious", and leads to exponential
        blow-up when done naively.  Early termination, depth-limiting, and
        condition reasoning are expected to handle this.
** Disjunctions
   - Although overloaded functions are interpreted as intersection types, actual
     inference must be done on case analysis
   - This is achieved by implementing CHR⋁, but creating tagged conjunctions for
     result types which are then merged "upwards" into the non-case-specific
     resulting effect
     - This could be what [gabrielli2009a] decribes as "Qualified Answers"

*** Subcontext matching
    - Disjunctions implemented with kind of justification mechanism.
    - New activation of constraint with context C1
      - matches store constraints in context C1
      - matches store constraints in parent context
      - in any case, newly created constraints must be also placed in C1
    - New activation of constraint in top context
      - matches store constraints in top context
      - matches store constraints in child context, but assumed context then
        has to be changed to the child context for the rest of the matching


    Semantics regarding propagaten/simplification:

    Constraints propagated by propagation rules are valid in the intersection of
    the matching heads' preconditions.  For this, matching can "specialize" the
    condition during matching:

    1. Anything that is valid in a more general context is also valid in a more
       specific context
    2. If anything followed logically from a more specific context, that is only
       universally valid in that specific context


    Thus, constraints to be removed by simplification rules(and possibly
    replaced), must be in the *most* specialized condition during building.
    I.e. we can build up an implication context during matching the kept heads,
    and this implication context must exactly match for all removed heads.
** Calls
   - Anonymous Functions (Quotations) are represented by inferred Effect types
   - Higher order functions occur in two ways, a "producer" way, and a
     "consumer" way
     - "producer" Whenever a quotation is pushed, it is inferred from the content
     - "consumer" Whenever a call is encountered, the called thing must be an
       effect compatible with the current stack
   - This results in a kind of "synchronisation" semantics: When calling a
     quotation, their effect vars are unified with the current stack to match
     the input, and the effect is re-inferred to take in information about the
     current environment.
** Variables
   Illustrative problem: ~max~ function

   - View 1: 2 inputs, 1 output, ~z = max(x,y)~.
   - View 2: 2 inputs, 1 conditional mux: ~(x > y ? x : y)~


   In the first case, the output value is distinct, and it is easy to define ~z~
   in terms of ~x~ and ~y~.  In the second case, we either select ~x~ or ~y~ as
   return value.

   The problem with the former: If we only consider the numerical effect of ~z~,
   we lose all information about ~x~ or ~y~ which is not related to the
   numerical comparison.

   The problem with the latter: we don't have a third variable representing the
   combined output properties.  Ergo, whenever something dependent is to be
   inferred, we would have to branch everything...
*** Strategy: Insert explicit muxes
    - This is tedious for yet unknown effects, as we need to keep track of
      muxing whole stacks?
    - This allows full data-flow semantics in predicate inference, but rules
      need to be specified on these mux predicates.
*** Strategy: Keep separate execution paths
    - This means never actually performing the phi's on type level, but
      accumulating more and more combinations of conditional effects.  This is
      the most precise, but may have exponential blow-up
** Role of syntactic types
   There is a 2-phase distinction during inference:
   1. match applicable stuff
   2. generate constraints because of that


   For the first part, we want to exclude any possible inference branch which
   cannot match.  For simple HM inference, this is achieved via non-unifying
   type terms, e.g. constructors, where their heads are taken from a set of
   independent base constructors.

   The same thing could also still be used for structural subtyping (recursive
   constructor term unification).

   With an underlying (compile-time-constant) subtype relation *between constructors* the unification
   can be extended semantically to check the corresponding subtype property at
   the call site.

   With semantic subtyping, it may be the case that we don't know yet whether
   the subtype property holds.  Especially when considering refinement types.

   In these cases, it is not possible to rely on preconditional relation checks
   only.

   The second part, i.e. constraint generation, is then a pure semantic
   operation between constraints, resulting in generated refinement predicates.

   However, in the general case of refinement type, it can be the case that it
   only becomes clear during inference that a considered branch is in fact
   invalid, (i.e. the corresponding types turned out not to be subtypes at the
   call site).  This can not be expressed in a pure syntactical way using
   matching at all.  This becomes obvious if we consider arbitrary predicate
   type definitions.
** Implementation Notes
Follows [chr-imperative] approach mostly, especially with regards to scheduling
using continuations.

* Type System
  - semantic (sub-)typing needed, since dynamic model
  - Intersection of Refinement types for functions
  - no recursive data type definitions
    - Allows for detection of recursive calls, because if a recursive type has
      been instantiated, it could only have been by circular call inference, not
      by instantiating a recursive template
      - Note: This mirrors the intuition that inductive type definitions are
        (especially) problematic, since they hide the actual computation
        required to build/access them (which are always finite in practice).


** Dependent types
   Typed Racket example, ~vector-ref~ function
   #+begin_src lisp
     (: vector-ref (∀ (A) (-> (Vectorof A) Integer A)))
     (define (vector-ref v i)
       (if (<= 0 i (sub1 (vector-length v)))
           (unsafe-vector-ref v i)
           (error 'vector-ref "invalid vector-index ~a" i))))
   #+end_src

   Supplanting ~unsafe-vector-ref~ with the following ~safe-vector-ref~:

   #+begin_src lisp
     (: safe-vector-ref
        (∀ {A} (-> ([v : (Vectorof A)]
                    [i : Integer])
                   #:pre (v i) (and (<= 0 i)
                                    (< i (vector-length v)))
                   A)))
     (define safe-vector-ref unsafe-vector-ref)
   #+end_src


** Semantic Subtyping
   main ref: [frisch2005semantic]
   - Set-Theoretic interpretation produces interesting subtype relation
     properties:
   - ~〚t1〛 ⊆ 〚t2〛 ⇐⇒ 〚t1〛 ∩ (D \〚t2〛) = ∅ ⇐⇒ 〚t1 ∧ ¬t2〛 = ∅~
     This means:
     1. ~t1~ is subtype of ~t2~ iff the set of runtime values represented by
        ~t1~ is a subset of the set of runtime values represented by ~t2~
     2. ~t1~ is a subtype of ~t2~ iff the set of values represented by ~t1~ does
        not intersect with the set of all possible runtime values except the
        ones represented by ~t2~.

   - Function type ~t1 -> t2~ set of binary relations ~f ⊆ D x {D + Ω}~, s.t.
     ~∀(d,d') ∊ f. d ∊ 〚t1〛=> d' ∊ 〚t2〛~

     Def: If ~D~ is set and ~X~, ~Y~ are subsets of ~D~, with ~D_o = {D + Ω}~,
     ~Ω~ being the error type:

     ~X → Y = {f ⊆ D x D_o | ∀(d,d'). d ∊ X => d' ∊ Y}~


* Compilation/Types
  - The basic mechanism is stateful inference, e.g. some kind of virtual
    execution/partial evaluation.
  - For Type predicates, all state-dependent information is dropped
  - For Compilation output, state-dependent stuff might still be important
** Effect Inference steps
   Input: Infer quot between stack a and b

   1. Step through quotation, returning a series of ~Is~ Predicates between
      stack definitions
      - The basic construct is ~Is{ ?c { call [ w1 w2 ] ?a } }~
      - This is decomposed into ~Is{ ?b { execute w1 } ?a }~ and
        ~Is{ ?c { call [ w2 ] ?b } }~ until the quotation is empty
      - On the ~execute~ level, things like conditionals are intercepted and
        handled specially
      - Also, the effect is applied to expand the input and output variables of
        the ~Is~ predicate structurally.
      - Eventually, the ~Is{ ?b { execute w } ?a }~ predicate is transformed
        into a ~Is{ ?b { w ?a } }~ form, denoting an expression instead of a
        transition, semantically.  The idea is that the predicate is now for
        value/type reasoning or definition collection, but not for partial
        execution anymore.
        There are several ways this is done
        - Direct handling because we are executing something special
        - Immediate Conversion because the word is primitive
        - If we have an inferred effect definition, this is applied to the
          inputs and outputs, and converted.
      - From a ~Is{ ?b { w ?a } }~ form, other expressions are derived.
        - Also, on that level, folding is checked.  This means that we do not jump
          back to the /execute/ level after values are known.  T
        - TODO: This has impliciations for e.g. if we have a word that is folded
          and returns somthing that is called, then we are not on the original
          execute context anymore.  However, we can trigger an inference on the
          returned quot, so this should not be a problem.
        - The most general variant on that level is a
          ~{ Expr y { foo { x1 x2 x3 } } }~  predicate.

**** Levels of equational theories
     - Builtin herbrand solver with ~==~ and ~==!~ words, performs
       (solver-branch-)scoped term equivalence.
     - ~{ Is y x }~ Predicates, which can exist in a conditional context.  Via
       ~ground-value-in-context~ it is possible to solve using all applicable
       ~Is~ predicates in the current context in addition to the (global)
       equational theory.  This is used for:
       - Branch balance checks after conditionals
       - Literal checks for foldable calls
       - Substituting ground values into ~{ Expr y ... }~ predicates.
*** Calls
    If a call is encountered, if the callee is literal, call the transition
    solver in-place.  The quot can be dropped and it's value is uninteresting
    (This is actually a special case of a foldable call in general).

    If the callee is non-literal, then we are actually /defining/ instead of
    consuming the effect.  The goal is then to infer an Effect predicate that
    will effectively be the constraint-level Simplification constraint for any
    subsequent calls, or the info on the return value, if applicable.
    There may be existing Effect definitions for this, which
    have to be applied.

    The order of steps is as follows:
    1. Perform all Effect instantiation rules on the stack pair at the call
       site.  After that, no Effect Predicates should be left.  This can split
       the state if the effect is polymorphic.
    2. Place an inference marker, which pulls in copies of all applicable predicates

*** Conditionals
    Branching is performed using the Split semantics of CHR-OR.  This means as
    soon as a conditional is encountered, the whole solver configuration is
    split.
    This is true for encountering the ~?~ word (or the ~if~ by extension), which
    branches the whole solver state.

    This is handled by the special ~Cond{ { Key1 Body1 } {Key2 Body2 } ... }~ Predicate, which effectively has
    access to the current continuation of the solver.  It will finish the
    solving for each case, and return any remaining predicates wrapped in a
    ~C{ Key Pred }~ construct.  The continuation of the parent execution is
    dropped.

    Different to pure CHR-OR semantics, after the branches are finished, the
    wrapped constraints are stored in context with the corresponding key.  After
    the main solver routine is finished, it will put all those back into the
    store at the same time.  Thus, all "leaves" appear in the same "horizontal"
    context at once.  The alternative would be to perform intermediate joins.
    However, the goal is to infer a "flat" set of types for each condition,
    Allowing to create an intersection type indexed by the corresponding
    choices.


**** Duplication of continutations
   On a split transaction, the complete solver queue is copied over.  This
   means, that any actions that should be performed in parent context are
   duplicated in each branch context.  When the corresponding joins happen,
   these will be duplicates.  Thus, care must be taken that these predicates
   have regular set semantics, as opposed to multi-set semantics to prevent
   repeated re-execution of rule matches.

** Conditional Effect Type hierarchy
   To manage split/join semantics:

   During main inference, all effects are unconditional, and thus their effect
   type predicates are filled with correctly scoped type predicates.


*** Strategy, expand-conditional-only
    During merging, we would then never touch the conditional effects again,
    only access their predicates to reason about summary conditions

** Intersection Arrow Types
   The split-store semantics causes all combinations of conditions to be
   returned at the end, where all conditional rules can be applied in parallel
   to infer the corresponding effects.  The resulting set is the polymorphic
   type.

** Subordinate solvers (TODO)
   - Subtype relation
   - Interval arithmetics
   - Linear (in-)equatuions for inferring loop bounds.
     - fourier-motzkin
     - possible ref: https://ths.rwth-aachen.de/wp-content/uploads/sites/4/kobialka_master.pdf
