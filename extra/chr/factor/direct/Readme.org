* Principles
** Basic Representation
   - From CHR-based typing: Translate expressions into predicates on types.
   - Difference: all types of words are binary, relating stacks to stacks
     (perhaps the same but basically always expanding the arrows?)
   -
** Types
   - Proposition-based typing: Refinement types (logical typing?)
   - Basic predicates about types, e.g. ~P{ Instance ?x number }
   - [logicdiss] Occurrence typing without semantic subtyping is crap

*** Quotations
    - represented by effect predicates, which act as instantiation templates
      when a call is registered.  These are inferred from the body.  Currently,
      all references to the internal variables should be removed once the effect
      is constructed
      - Reasoning back into the quotation constraints is thus not really possible
        after it has been created.
      - TODO: Currying?
    - Duplication is currently handled symbolically, e.g. duplication of a
      callable is referential, and any per-call differentiation happens via
      instantiation
*** Intersection/Union semantics
    - Set of constraints is a conjunction of predicates, so there are two
      possibilities to interpret something like
      Type(x) = integer ∧ Type(x) = string
      1. The variable x must be of the intersection type integer∧string
      2. The variable x must be able to hold values of type integer as well as
         values of type string.


      This is a basic conflict of meaning: "The value must be both a string and
      an integer" vs. "The variable must be able to hold both a string and an integer".


      In a modal interpretation, this could maybe be read as, respectively:
      1. Possible(Type(x) = integer) ∧ Possible(Type(x) = string)
      2. Necessary(Type(x) = (integer∪string))


      Without introducing ambiguities in the set semantics.  This would make
      sense if the types as value-sets specify the value range, while "building"
      up the type is unterstood as a composition of bottom-bounded sets?

      The different ways to construct these is already defined by the different
      ways to interpret the presence of multiple effect definitions.
      Declarations serve to actively cut the possiblity space, while alternative
      executions build up the possibility space.

      Approach:
      - Interpret conjunction of declaration constraints as intersection types
      - Interpret conjunction of instance constraints as union-creating types


      From the conditional structure, could we "force" unions to be indexed?
*** Occurrence/Refinement Typing
    Big difference to "regular" type-logic connections:
    types, propositions and terms are distinct.  Propositions state claims about
    runtime environment, relating types and variables.


    - Approach from [tobin-hochstadt2010logical_types]: Propositional types
    - For each type, a propositional expression containing the following
      information is stated:
      - Type: T, N, #t, #f, union(⋃)
      - latent propositions:
        - case 1: expression evaluates to boolean true
        - case 2: expression evaluates to boolean false
        - Propositions can be tt, ff, atomic: type_(x), implication,
          conjunction, disjunction
        - *object* This one is interesting.  It defines how to access the part
          of the runtime environment which the proposition refers to.  Base
          system: variable, empty object
**** RTR variant:
     - expression is assigned type-result e: <τ, p, q, o> where
       - e has type τ
       - if e evaluates to non-false, then p holds
       - if e evaluates to false, then q holds
       - e's value corresponds to symbolic object o
* Constraint Handling Rules
  - used to express properties about types
  - Most basic: Effect types
    - Declare the input/output parms of a function
    - optionally tagged with a conditional case for keeping track of type
      disjunctions
    - "Body" containing constraints about the parameter
    - effectively a ∀i,o:... construct
    - The original approach is (ref chr-type-paper) to translate the program
      into rules for each occurrence.  Important: These are simplification rules
      at the call sites
      - This implementation originally also did that by computing the
        simplification rules on the fly, but switched to simulating the
        simplification instantiation
      - One reason: bi-directionality.  We want to be able to add information
        about an instantiation rule after a function is encountered (Most
        important example: currying).
      - Other problem: overloading behavior. (See also [stuckey2005], but for
        static typing).  We effectively infer different effect types (which
        correspond to different rules) for different possibilities of
        execution.  For static typing, the correct one is chosen based on the
        shape of the argument term.  However, this only works for syntactic
        typing, where the argument type is completely determined by the
        structure of the term.  For semantic subtyping, whether a rule would
        succeed can only be seen by testing the instantiation, hence the need
        for CHR⋁ semantics.
      - In other words, it can be the case that while during runtime, only one
        version will be executed, during compile time, we must deal with the
        fact that we don't know which one.  Thus, we consider all applicable
        ones.  Note that this property is "contagious", and leads to exponential
        blow-up when done naively.  Early termination, depth-limiting, and
        condition reasoning are expected to handle this.
** Disjunctions
   - Although overloaded functions are interpreted as intersection types, actual
     inference must be done on case analysis
   - This is achieved by implementing CHR⋁, but creating tagged conjunctions for
     result types which are then merged "upwards" into the non-case-specific
     resulting effect
     - This could be what [gabrielli2009a] decribes as "Qualified Answers"
** Calls
   - Anonymous Functions (Quotations) are represented by inferred Effect types
   - Higher order functions occur in two ways, a "producer" way, and a
     "consumer" way
     - "producer" Whenever a quotation is pushed, it is inferred from the content
     - "consumer" Whenever a call is encountered, the called thing must be an
       effect compatible with the current stack
   - This results in a kind of "synchronisation" semantics: When calling a
     quotation, their effect vars are unified with the current stack to match
     the input, and the effect is re-inferred to take in information about the
     current environment.
** Variables
   Illustrative problem: ~max~ function

   - View 1: 2 inputs, 1 output, ~z = max(x,y)~.
   - View 2: 2 inputs, 1 conditional mux: ~(x > y ? x : y)~


   In the first case, the output value is distinct, and it is easy to define ~z~
   in terms of ~x~ and ~y~.  In the second case, we either select ~x~ or ~y~ as
   return value.

   The problem with the former: If we only consider the numerical effect of ~z~,
   we lose all information about ~x~ or ~y~ which is not related to the
   numerical comparison.

   The problem with the latter: we don't have a third variable representing the
   combined output properties.  Ergo, whenever something dependent is to be
   inferred, we would have to branch everything...
*** Strategy: Insert explicit muxes
    - This is tedious for yet unknown effects, as we need to keep track of
      muxing whole stacks?
    - This allows full data-flow semantics in predicate inference, but rules
      need to be specified on these mux predicates.
*** Strategy: Keep separate execution paths
    - This means never actually performing the phi's on type level, but
      accumulating more and more combinations of conditional effects.  This is
      the most precise, but may have exponential blow-up
** Role of syntactic types
   There is a 2-phase distinction during inference:
   1. match applicable stuff
   2. generate constraints because of that


   For the first part, we want to exclude any possible inference branch which
   cannot match.  For simple HM inference, this is achieved via non-unifying
   type terms, e.g. constructors, where their heads are taken from a set of
   independent base constructors.

   The same thing could also still be used for structural subtyping (recursive
   constructor term unification).

   With an underlying (compile-time-constant) subtype relation *between constructors* the unification
   can be extended semantically to check the corresponding subtype property at
   the call site.

   With semantic subtyping, it may be the case that we don't know yet whether
   the subtype property holds.  Especially when considering refinement types.

   In these cases, it is not possible to rely on preconditional relation checks
   only.

   The second part, i.e. constraint generation, is then a pure semantic
   operation between constraints, resulting in generated refinement predicates.

   However, in the general case of refinement type, it can be the case that it
   only becomes clear during inference that a considered branch is in fact
   invalid, (i.e. the corresponding types turned out not to be subtypes at the
   call site).  This can not be expressed in a pure syntactical way using
   matching at all.  This becomes obvious if we consider arbitrary predicate
   type definitions.


* Type System
  - semantic (sub-)typing needed, since dynamic model
  - Intersection of Refinement types for functions
  - no recursive data type definitions
    - Allows for detection of recursive calls, because if a recursive type has
      been instantiated, it could only have been by circular call inference, not
      by instantiating a recursive template
      - Note: This mirrors the intuition that inductive type definitions are
        (especially) problematic, since they hide the actual computation
        required to build/access them (which are always finite in practice).


** Dependent types
   Typed Racket example, ~vector-ref~ function
   #+begin_src lisp
     (: vector-ref (∀ (A) (-> (Vectorof A) Integer A)))
     (define (vector-ref v i)
       (if (<= 0 i (sub1 (vector-length v)))
           (unsafe-vector-ref v i)
           (error 'vector-ref "invalid vector-index ~a" i))))
   #+end_src

   Supplanting ~unsafe-vector-ref~ with the following ~safe-vector-ref~:

   #+begin_src lisp
     (: safe-vector-ref
        (∀ {A} (-> ([v : (Vectorof A)]
                    [i : Integer])
                   #:pre (v i) (and (<= 0 i)
                                    (< i (vector-length v)))
                   A)))
     (define safe-vector-ref unsafe-vector-ref)
   #+end_src


** Semantic Subtyping
   main ref: [frisch2005semantic]
   - Set-Theoretic interpretation produces interesting subtype relation
     properties:
   - ~〚t1〛 ⊆ 〚t2〛 ⇐⇒ 〚t1〛 ∩ (D \〚t2〛) = ∅ ⇐⇒ 〚t1 ∧ ¬t2〛 = ∅~
     This means:
     1. ~t1~ is subtype of ~t2~ iff the set of runtime values represented by
        ~t1~ is a subset of the set of runtime values represented by ~t2~
     2. ~t1~ is a subtype of ~t2~ iff the set of values represented by ~t1~ does
        not intersect with the set of all possible runtime values except the
        ones represented by ~t2~.

   - Function type ~t1 -> t2~ set of binary relations ~f ⊆ D x {D + Ω}~, s.t.
     ~∀(d,d') ∊ f. d ∊ 〚t1〛=> d' ∊ 〚t2〛~

     Def: If ~D~ is set and ~X~, ~Y~ are subsets of ~D~, with ~D_o = {D + Ω}~,
     ~Ω~ being the error type:

     ~X → Y = {f ⊆ D x D_o | ∀(d,d'). d ∊ X => d' ∊ Y}~


* Compilation/Types
  - The basic mechanism is stateful inference, e.g. some kind of virtual
    execution/partial evaluation.
  - For Type predicates, all state-dependent information is dropped
  - For Compilation output, state-dependent stuff might still be important
** Effect Inference steps
   Input: Infer quot between stack a and b

   1. Call the Transition solver
      1. Step through quotation, returning a series of Eval Predicates
      2. Mark/Sweep the remaining Stack definitions
   2. Perform constraint completion
      1. Run through all Eval predicates, and fill the store with value constraints
      2. In parallel, all kinds of subordinate solvers will be active to
         simplify/complete
   3. Start the Effect solver
      1. Put an infer-effect request between stacks a and b
      2. Collect all predicates that are described by the bound variables,
         returning an Effect pred.
      3. Clean up any remaining predicates that are covered by the scope of the
         Effect between a and b.

*** Calls
    If a call is encountered, if the callee is literal, call the transition
    solver in-place.  The quot can be dropped and it's value is uninteresting
    (This is actually a special case of a foldable call in general).

    If the callee is non-literal, then we are actually /defining/ instead of
    consuming the effect.  The goal is then to infer an Effect predicate that
    will effectively be the constraint-level Simplification constraint for any
    subsequent calls, or the info on the return value, if applicable.
    There may be existing Effect definitions for this, which
    have to be applied.

    The order of steps is as follows:
    1. Perform all Effect instantiation rules on the stack pair at the call
       site.  After that, no Effect Predicates should be left.  This can split
       the state if the effect is polymorphic.
    2. Place an inference marker, which pulls in copies of all applicable predicates

*** Conditionals
    Branching is performed using the Split semantics of CHR-OR.  This means as
    soon as a conditional is encountered, the whole solver configuration is
    split.
    This is true for encountering the ~?~ word (or the ~if~ by extension), which
    branches the whole solver state.

    This is handled by the special ~Cond{ { Key1 Body1 } {Key2 Body2 } ... }~ Predicate, which effectively has
    access to the current continuation of the solver.  It will finish the
    solving for each case, and return any remaining predicates wrapped in a
    ~C{ Key Pred }~ construct.  The continuation of the parent execution is
    dropped.

    Different to pure CHR-OR semantics, after the branches are finished, the
    wrapped constraints are stored in context with the corresponding key.  After
    the main solver routine is finished, it will put all those back into the
    store at the same time.  Thus, all "leaves" appear in the same "horizontal"
    context at once.  The alternative would be to perform intermediate joins.
    However, the goal is to infer a "flat" set of types for each condition,
    Allowing to create an intersection type indexed by the corresponding
    choices.


**** Duplication of continutations
   On a split transaction, the complete solver queue is copied over.  This
   means, that any actions that should be performed in parent context are
   duplicated in each branch context.  When the corresponding joins happen,
   these will be duplicates.  Thus, care must be taken that these predicates
   have regular set semantics, as opposed to multi-set semantics to prevent
   repeated re-execution of rule matches.

** Conditional Effect Type hierarchy
   To manage split/join semantics:

   During main inference, all effects are unconditional, and thus their effect
   type predicates are filled with correctly scoped type predicates.


*** Strategy, expand-conditional-only
    During merging, we would then never touch the conditional effects again,
    only access their predicates to reason about summary conditions

** Intersection Arrow Types
   The split-store semantics causes all combinations of conditions to be
   returned at the end, where all conditional rules can be applied in parallel
   to infer the corresponding effects.  The resulting set is the polymorphic
   type.

** Subordinate solvers (TODO)
   - Subtype relation
   - Interval arithmetics
   - Linear (in-)equatuions for inferring loop bounds.
     - fourier-motzkin
     - possible ref: https://ths.rwth-aachen.de/wp-content/uploads/sites/4/kobialka_master.pdf
